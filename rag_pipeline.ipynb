{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99c26c92",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e29174a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\GenAIPractise\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "from langchain_community.document_loaders import PyPDFLoader,PyMuPDFLoader\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "from langchain_community.document_loaders.excel import UnstructuredExcelLoader\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "254103f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3 documents from PDF.\n"
     ]
    }
   ],
   "source": [
    "data_path = Path(\"data\")\n",
    "pdf_files = list(data_path.rglob(\"*.pdf\"))\n",
    "loader = PyPDFLoader(str(pdf_files[0]))\n",
    "documents = loader.load()\n",
    "print(f\"Loaded {len(documents)} documents from PDF.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04899fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load PDF Files \n",
    "def load_pdfs_from_directory(data_dir: str) -> List[Any]:\n",
    "    \"\"\" Load all pdf files from the data directory\n",
    "        Supported formats: .pdf\n",
    "    \"\"\"\n",
    "    data_path = Path(data_dir)\n",
    "    pdf_files = list(data_path.rglob(\"*.pdf\"))\n",
    "    all_documents = []\n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"Loading PDF file: {pdf_file}\")\n",
    "        loader = PyMuPDFLoader(str(pdf_file))\n",
    "        documents = loader.load()\n",
    "        all_documents.extend(documents)\n",
    "        print(f\"Loaded {len(documents)} documents from {pdf_file}\")\n",
    "    return all_documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c0141f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PDF file: data\\AgenticRAG.pdf\n",
      "Loaded 3 documents from data\\AgenticRAG.pdf\n",
      "Loading PDF file: data\\Linear Regression.pdf\n",
      "Loaded 2 documents from data\\Linear Regression.pdf\n"
     ]
    }
   ],
   "source": [
    "docs = load_pdfs_from_directory(data_dir=\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0054181f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLMs enable the development of intelligent agents capable of tackling \n",
      "complex, non-repetitive tasks that defy description as deterministic \n",
      "workflows. By spli_ing reasoning into steps in different ways and \n",
      "orchestrating them in a relatively simple way, agents can demonstrate a \n",
      "significantly higher task completion rate on complex open tasks. \n",
      "This agent-based approach can be applied across numerous domains, \n",
      "including RAG systems, which we discussed in Chapter 4. As a reminder, \n",
      "what exactly is agentic RAG? Remember, a classic pa_ern for a RAG system is \n",
      "to retrieve chunks given the query, combine them into the context, and ask an \n",
      "LLM to generate an answer given a system prompt, combined context, and the \n",
      "question. \n",
      "We can improve each of these steps using the principles discussed above \n",
      "(decomposition, tool calling, and adaptation): \n",
      "Dynamic retrieval hands over the retrieval query generation to the LLM. \n",
      "It can decide itself whether to use sparse embeddings, hybrid methods, \n",
      "keyword search, or web search. You can wrap retrievals as tools and \n",
      "orchestrate them as a LangGraph graph. \n",
      "Query expansion tasks an LLM to generate multiple queries based on \n",
      "initial ones, and then you combine search outputs based on reciprocal \n",
      "fusion or another technique. \n",
      "Decomposition of reasoning on retrieved chunks allows you to ask an \n",
      "LLM to evaluate each individual chunk given the question (and filter it \n",
      "out if it’s irrelevant) to compensate for retrieval inaccuracies. Or you can \n",
      "ask an LLM to summarize each chunk by keeping only information given \n",
      "for the input question. Anyway, instead of throwing a huge piece of \n",
      "context in front of an LLM, you perform many smaller reasoning steps in \n",
      "parallel first.This can not only improve the RAG quality by itself but also \n",
      "increase the amount of initially retrieved chunks (by decreasing the \n",
      "relevance threshold) or expand each individual chunk with its neighbors. \n",
      "In other words, you can overcome some retrieval challenges with LLM \n",
      "reasoning. It might increase the overall performance of your application,\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba4b837f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d1aaec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=100,separators=[\"\\n\\n\",\"\\n\",\" \",\"\"])\n",
    "chunks=splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d08e891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LLMs enable the development of intelligent agents capable of tackling \\ncomplex, non-repetitive tasks that defy description as deterministic \\nworkflows. By spli_ing reasoning into steps in different ways and \\norchestrating them in a relatively simple way, agents can demonstrate a \\nsignificantly higher task completion rate on complex open tasks. \\nThis agent-based approach can be applied across numerous domains, \\nincluding RAG systems, which we discussed in Chapter 4. As a reminder, \\nwhat exactly is agentic RAG? Remember, a classic pa_ern for a RAG system is \\nto retrieve chunks given the query, combine them into the context, and ask an \\nLLM to generate an answer given a system prompt, combined context, and the \\nquestion. \\nWe can improve each of these steps using the principles discussed above \\n(decomposition, tool calling, and adaptation): \\nDynamic retrieval hands over the retrieval query generation to the LLM. \\nIt can decide itself whether to use sparse embeddings, hybrid methods,', 'It can decide itself whether to use sparse embeddings, hybrid methods, \\nkeyword search, or web search. You can wrap retrievals as tools and \\norchestrate them as a LangGraph graph. \\nQuery expansion tasks an LLM to generate multiple queries based on \\ninitial ones, and then you combine search outputs based on reciprocal \\nfusion or another technique. \\nDecomposition of reasoning on retrieved chunks allows you to ask an \\nLLM to evaluate each individual chunk given the question (and filter it \\nout if it’s irrelevant) to compensate for retrieval inaccuracies. Or you can \\nask an LLM to summarize each chunk by keeping only information given \\nfor the input question. Anyway, instead of throwing a huge piece of \\ncontext in front of an LLM, you perform many smaller reasoning steps in \\nparallel first.This can not only improve the RAG quality by itself but also \\nincrease the amount of initially retrieved chunks (by decreasing the', 'increase the amount of initially retrieved chunks (by decreasing the \\nrelevance threshold) or expand each individual chunk with its neighbors. \\nIn other words, you can overcome some retrieval challenges with LLM \\nreasoning. It might increase the overall performance of your application,', 'but of course, it comes with latency and potential cost implications. \\nReflection steps and iterations task LLMs to dynamically iterate on \\nretrieval and query expansion by evaluating the outputs after each \\niteration. You can also use additional grounding and a_ribution tools as a \\nseparate step in your workflow and, based on that, reason whether you \\nneed to continue working on the answer or the answer can be returned to \\nthe user. \\nBased on our definition from the previous chapters, RAG becomes agentic \\nRAG when you have shared partial control with the LLM over the execution \\nflow. For example, if the LLM decides how to retrieve, reflects on retrieved \\nchunks, and adapts based on the first version of the answer, it becomes \\nagentic RAG. From our perspective, at this point, it starts making sense to \\nmigrate to LangGraph since it’s designed specifically for building such \\napplications, but of course, you can stay with LangChain or any other', 'applications, but of course, you can stay with LangChain or any other \\nframework you prefer (compare how we implemented map-reduce video \\nsummarization with LangChain and LangGraph separately in Chapter 3). \\nMulti-agent architectures \\nIn Chapter 5, we learned that decomposing a complex task into simpler \\nsubtasks typically increases LLM performance. We built a plan-and-solve \\nagent that goes a step further than CoT and encourages the LLM to generate a \\nplan and follow it. To a certain extent, this architecture was a multi-agent one \\nsince the research agent (which was responsible for generating and following \\nthe plan) invoked another agent that focused on a different type of task – \\nsolving very specific tasks with provided tools. Multi-agentic workflows \\norchestrate multiple agents, allowing them to enhance each other and at the \\nsame time keep agents modular (which makes it easier to test and reuse \\nthem).', 'same time keep agents modular (which makes it easier to test and reuse \\nthem). \\nWe will look into a few core agentic architectures in the remainder of this \\nchapter, and introduce some important LangGraph interfaces (such as \\nstreaming details and handoffs) that are useful to develop agents. If you’re \\ninterested, you can find more examples and tutorials on the LangChain', 'documentation page at https://langchainai. \\ngithub.io/langgraph/tutorials/#agent-architectures. We’ll begin \\nwith discussing the importance of specialization in multi-agentic systems, \\nincluding what the consensus mechanism is and the different consensus \\nmechanisms. \\nAgent roles and specialization \\nWhen working on a complex task, we as humans know that usually, it’s \\nbeneficial to have a team with diverse skills and backgrounds. There is much \\nevidence from research and experiments that suggests this is also true for \\ngenerative AI agents. In fact, developing specialized agents offers several \\nadvantages for complex AI systems. \\nFirst, specialization improves performance on specific tasks. This allows you \\nto: \\nSelect the optimal set of tools for each task type. \\nCraft tailored prompts and workflows. \\nFine-tune hyperparameters such as temperature for specific contexts. \\nSecond, specialized agents help manage complexity. Current LLMs struggle', 'Second, specialized agents help manage complexity. Current LLMs struggle \\nwhen handling too many tools at once. As a best practice, limit each agent to \\n5-15 different tools, rather than overloading a single agent with all available \\ntools. How to group tools is still an open question; typically, grouping them \\ninto toolkits to create coherent specialized agents helps.', 'Linear Regression (No Code Guide) \\n1. Introduction to Linear Regression \\n• \\nDefinition: Linear regression is a statistical method used to model the relationship between a \\ndependent variable and one or more independent variables. \\n• \\nPurpose: It helps in predicting outcomes and understanding how variables are related. \\n• \\nApplications: Business forecasting, risk analysis, economics, medical research, and machine \\nlearning. \\n \\n2. Core Concepts \\n• \\nDependent Variable (Y): The outcome you want to predict. \\n• \\nIndependent Variable (X): The input or predictor. \\n• \\nLine of Best Fit: A straight line that minimizes the difference between predicted and actual \\nvalues. \\n• \\nEquation: ( y = mx + c )  \\no \\n(m): slope (effect of X on Y) \\no \\n(c): intercept (value of Y when X = 0) \\n \\n3. Assumptions of Linear Regression \\n• \\nLinearity: Relationship between X and Y is linear. \\n• \\nIndependence: Observations are independent of each other. \\n• \\nHomoscedasticity: Constant variance of errors. \\n•', '• \\nHomoscedasticity: Constant variance of errors. \\n• \\nNormality: Residuals (errors) are normally distributed. \\n \\n4. Types of Linear Regression \\n• \\nSimple Linear Regression: One independent variable. \\n• \\nMultiple Linear Regression: Two or more independent variables. \\n \\n5. Evaluation Metrics \\n• \\nR-squared (R²): Explains how much variance in Y is explained by X. \\n• \\nMean Squared Error (MSE): Average squared difference between predicted and actual \\nvalues. \\n• \\nRoot Mean Squared Error (RMSE): Square root of MSE, easier to interpret.', '6. Advantages and Limitations \\n• \\nAdvantages:  \\no \\nEasy to understand and implement \\no \\nWorks well for simple relationships \\no \\nProvides interpretable coefficients \\n• \\nLimitations:  \\no \\nAssumes linearity (not suitable for complex patterns) \\no \\nSensitive to outliers \\no \\nRequires assumptions to be met for reliable results \\n \\n7. Conclusion \\nLinear regression is a foundational tool in statistics and machine learning. While simple, it provides \\npowerful insights into relationships between variables and serves as a starting point for more \\nadvanced modeling techniques.']\n"
     ]
    }
   ],
   "source": [
    "texts=[chunk.page_content for chunk in chunks]\n",
    "print(texts)\n",
    "# embeddings=SentenceTransformer(model_name_or_path==\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bdb974c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:04<00:00,  4.06s/it]\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embeddings=model.encode(texts,show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3b264f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.01537459, -0.00807518, -0.01349245, ...,  0.05261908,\n",
       "        -0.02061925,  0.05256796],\n",
       "       [ 0.01792927,  0.0146074 ,  0.01082158, ...,  0.01337339,\n",
       "        -0.07734995,  0.03242376],\n",
       "       [ 0.0203466 ,  0.01087702, -0.00036829, ..., -0.01249124,\n",
       "        -0.087068  ,  0.01081242],\n",
       "       ...,\n",
       "       [-0.05097468,  0.0071922 , -0.06002007, ..., -0.00390866,\n",
       "         0.04024328, -0.06602587],\n",
       "       [ 0.08606234, -0.06391905,  0.01570605, ...,  0.0404456 ,\n",
       "         0.07214393, -0.04788442],\n",
       "       [-0.04269894,  0.00727912, -0.01060689, ...,  0.01063519,\n",
       "         0.07772786, -0.02818427]], shape=(11, 384), dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embeddings.shape[1]\n",
    "emb1=np.array(embeddings.astype(\"float32\"))\n",
    "emb1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2bc7b433",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79be2f5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'LLMs enable the development of intelligent agents capable of tackling \\ncomplex, non-repetitive tasks that defy description as deterministic \\nworkflows. By spli_ing reasoning into steps in different ways and \\norchestrating them in a relatively simple way, agents can demonstrate a \\nsignificantly higher task completion rate on complex open tasks. \\nThis agent-based approach can be applied across numerous domains, \\nincluding RAG systems, which we discussed in Chapter 4. As a reminder, \\nwhat exactly is agentic RAG? Remember, a classic pa_ern for a RAG system is \\nto retrieve chunks given the query, combine them into the context, and ask an \\nLLM to generate an answer given a system prompt, combined context, and the \\nquestion. \\nWe can improve each of these steps using the principles discussed above \\n(decomposition, tool calling, and adaptation): \\nDynamic retrieval hands over the retrieval query generation to the LLM. \\nIt can decide itself whether to use sparse embeddings, hybrid methods,'},\n",
       " {'text': 'It can decide itself whether to use sparse embeddings, hybrid methods, \\nkeyword search, or web search. You can wrap retrievals as tools and \\norchestrate them as a LangGraph graph. \\nQuery expansion tasks an LLM to generate multiple queries based on \\ninitial ones, and then you combine search outputs based on reciprocal \\nfusion or another technique. \\nDecomposition of reasoning on retrieved chunks allows you to ask an \\nLLM to evaluate each individual chunk given the question (and filter it \\nout if it’s irrelevant) to compensate for retrieval inaccuracies. Or you can \\nask an LLM to summarize each chunk by keeping only information given \\nfor the input question. Anyway, instead of throwing a huge piece of \\ncontext in front of an LLM, you perform many smaller reasoning steps in \\nparallel first.This can not only improve the RAG quality by itself but also \\nincrease the amount of initially retrieved chunks (by decreasing the'},\n",
       " {'text': 'increase the amount of initially retrieved chunks (by decreasing the \\nrelevance threshold) or expand each individual chunk with its neighbors. \\nIn other words, you can overcome some retrieval challenges with LLM \\nreasoning. It might increase the overall performance of your application,'},\n",
       " {'text': 'but of course, it comes with latency and potential cost implications. \\nReflection steps and iterations task LLMs to dynamically iterate on \\nretrieval and query expansion by evaluating the outputs after each \\niteration. You can also use additional grounding and a_ribution tools as a \\nseparate step in your workflow and, based on that, reason whether you \\nneed to continue working on the answer or the answer can be returned to \\nthe user. \\nBased on our definition from the previous chapters, RAG becomes agentic \\nRAG when you have shared partial control with the LLM over the execution \\nflow. For example, if the LLM decides how to retrieve, reflects on retrieved \\nchunks, and adapts based on the first version of the answer, it becomes \\nagentic RAG. From our perspective, at this point, it starts making sense to \\nmigrate to LangGraph since it’s designed specifically for building such \\napplications, but of course, you can stay with LangChain or any other'},\n",
       " {'text': 'applications, but of course, you can stay with LangChain or any other \\nframework you prefer (compare how we implemented map-reduce video \\nsummarization with LangChain and LangGraph separately in Chapter 3). \\nMulti-agent architectures \\nIn Chapter 5, we learned that decomposing a complex task into simpler \\nsubtasks typically increases LLM performance. We built a plan-and-solve \\nagent that goes a step further than CoT and encourages the LLM to generate a \\nplan and follow it. To a certain extent, this architecture was a multi-agent one \\nsince the research agent (which was responsible for generating and following \\nthe plan) invoked another agent that focused on a different type of task – \\nsolving very specific tasks with provided tools. Multi-agentic workflows \\norchestrate multiple agents, allowing them to enhance each other and at the \\nsame time keep agents modular (which makes it easier to test and reuse \\nthem).'},\n",
       " {'text': 'same time keep agents modular (which makes it easier to test and reuse \\nthem). \\nWe will look into a few core agentic architectures in the remainder of this \\nchapter, and introduce some important LangGraph interfaces (such as \\nstreaming details and handoffs) that are useful to develop agents. If you’re \\ninterested, you can find more examples and tutorials on the LangChain'},\n",
       " {'text': 'documentation page at https://langchainai. \\ngithub.io/langgraph/tutorials/#agent-architectures. We’ll begin \\nwith discussing the importance of specialization in multi-agentic systems, \\nincluding what the consensus mechanism is and the different consensus \\nmechanisms. \\nAgent roles and specialization \\nWhen working on a complex task, we as humans know that usually, it’s \\nbeneficial to have a team with diverse skills and backgrounds. There is much \\nevidence from research and experiments that suggests this is also true for \\ngenerative AI agents. In fact, developing specialized agents offers several \\nadvantages for complex AI systems. \\nFirst, specialization improves performance on specific tasks. This allows you \\nto: \\nSelect the optimal set of tools for each task type. \\nCraft tailored prompts and workflows. \\nFine-tune hyperparameters such as temperature for specific contexts. \\nSecond, specialized agents help manage complexity. Current LLMs struggle'},\n",
       " {'text': 'Second, specialized agents help manage complexity. Current LLMs struggle \\nwhen handling too many tools at once. As a best practice, limit each agent to \\n5-15 different tools, rather than overloading a single agent with all available \\ntools. How to group tools is still an open question; typically, grouping them \\ninto toolkits to create coherent specialized agents helps.'},\n",
       " {'text': 'Linear Regression (No Code Guide) \\n1. Introduction to Linear Regression \\n• \\nDefinition: Linear regression is a statistical method used to model the relationship between a \\ndependent variable and one or more independent variables. \\n• \\nPurpose: It helps in predicting outcomes and understanding how variables are related. \\n• \\nApplications: Business forecasting, risk analysis, economics, medical research, and machine \\nlearning. \\n \\n2. Core Concepts \\n• \\nDependent Variable (Y): The outcome you want to predict. \\n• \\nIndependent Variable (X): The input or predictor. \\n• \\nLine of Best Fit: A straight line that minimizes the difference between predicted and actual \\nvalues. \\n• \\nEquation: ( y = mx + c )  \\no \\n(m): slope (effect of X on Y) \\no \\n(c): intercept (value of Y when X = 0) \\n \\n3. Assumptions of Linear Regression \\n• \\nLinearity: Relationship between X and Y is linear. \\n• \\nIndependence: Observations are independent of each other. \\n• \\nHomoscedasticity: Constant variance of errors. \\n•'},\n",
       " {'text': '• \\nHomoscedasticity: Constant variance of errors. \\n• \\nNormality: Residuals (errors) are normally distributed. \\n \\n4. Types of Linear Regression \\n• \\nSimple Linear Regression: One independent variable. \\n• \\nMultiple Linear Regression: Two or more independent variables. \\n \\n5. Evaluation Metrics \\n• \\nR-squared (R²): Explains how much variance in Y is explained by X. \\n• \\nMean Squared Error (MSE): Average squared difference between predicted and actual \\nvalues. \\n• \\nRoot Mean Squared Error (RMSE): Square root of MSE, easier to interpret.'},\n",
       " {'text': '6. Advantages and Limitations \\n• \\nAdvantages:  \\no \\nEasy to understand and implement \\no \\nWorks well for simple relationships \\no \\nProvides interpretable coefficients \\n• \\nLimitations:  \\no \\nAssumes linearity (not suitable for complex patterns) \\no \\nSensitive to outliers \\no \\nRequires assumptions to be met for reliable results \\n \\n7. Conclusion \\nLinear regression is a foundational tool in statistics and machine learning. While simple, it provides \\npowerful insights into relationships between variables and serves as a starting point for more \\nadvanced modeling techniques.'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata=[{\"text\":chunk.page_content} for chunk in chunks]\n",
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d11b8d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating index for FAISS vector store using L2 distance metric same embeddings shape\n",
    "\n",
    "index=faiss.IndexFlatL2(embeddings.shape[1])\n",
    "index.add(np.array(embeddings.astype(\"float32\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aee550ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index and metadata saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Store the index and metadata to disk\n",
    "\n",
    "os.makedirs(\"faiss_index\",exist_ok=True)\n",
    "faiss_path=\"faiss_index/faiss.index\"\n",
    "faiss.write_index(index,faiss_path)\n",
    "metadata_path=\"faiss_index/metadata.pkl\"\n",
    "with open (metadata_path,\"wb\") as f:\n",
    "    pickle.dump(metadata,f)\n",
    "print(\"FAISS index and metadata saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e6d2967",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_index = faiss.read_index(faiss_path)\n",
    "with open(metadata_path,\"rb\") as f:\n",
    "    loaded_metadata=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d53d3c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.52223206 0.8408511  0.9833946 ] [ 8 10  9]\n"
     ]
    }
   ],
   "source": [
    "queryemb = model.encode([\"What is Linear Regression?\"]).astype(\"float32\")\n",
    "D, I = load_index.search(np.array(queryemb),k=3)\n",
    "print(D[0],I[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf44412a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(D,I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "593932bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is Multi Agent Architecture?\"\n",
    "queryemb = model.encode([query]).astype(\"float32\")\n",
    "D, I = load_index.search(np.array(queryemb),k=3)\n",
    "results=[]\n",
    "for idx,distance in zip(I[0],D[0]):\n",
    "    meta = loaded_metadata[idx]\n",
    "    result={\"document\":meta[\"text\"],\"distance\":distance,\"index\":idx}\n",
    "    # print(result)\n",
    "    results.append(result)\n",
    "# print(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eaf9b3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "58ed908a",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "chat_groq_llm = ChatGroq(groq_api_key=groq_api_key, model=\"llama-3.1-8b-instant\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "733079e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts=[]\n",
    "texts.append([results[i]['document'] for i in range(len(results))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "59ea3658",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Linear Regression (No Code Guide) \\n1. Introduction to Linear Regression \\n• \\nDefinition: Linear regression is a statistical method used to model the relationship between a \\ndependent variable and one or more independent variables. \\n• \\nPurpose: It helps in predicting outcomes and understanding how variables are related. \\n• \\nApplications: Business forecasting, risk analysis, economics, medical research, and machine \\nlearning. \\n \\n2. Core Concepts \\n• \\nDependent Variable (Y): The outcome you want to predict. \\n• \\nIndependent Variable (X): The input or predictor. \\n• \\nLine of Best Fit: A straight line that minimizes the difference between predicted and actual \\nvalues. \\n• \\nEquation: ( y = mx + c )  \\no \\n(m): slope (effect of X on Y) \\no \\n(c): intercept (value of Y when X = 0) \\n \\n3. Assumptions of Linear Regression \\n• \\nLinearity: Relationship between X and Y is linear. \\n• \\nIndependence: Observations are independent of each other. \\n• \\nHomoscedasticity: Constant variance of errors. \\n•',\n",
       "  '6. Advantages and Limitations \\n• \\nAdvantages:  \\no \\nEasy to understand and implement \\no \\nWorks well for simple relationships \\no \\nProvides interpretable coefficients \\n• \\nLimitations:  \\no \\nAssumes linearity (not suitable for complex patterns) \\no \\nSensitive to outliers \\no \\nRequires assumptions to be met for reliable results \\n \\n7. Conclusion \\nLinear regression is a foundational tool in statistics and machine learning. While simple, it provides \\npowerful insights into relationships between variables and serves as a starting point for more \\nadvanced modeling techniques.',\n",
       "  '• \\nHomoscedasticity: Constant variance of errors. \\n• \\nNormality: Residuals (errors) are normally distributed. \\n \\n4. Types of Linear Regression \\n• \\nSimple Linear Regression: One independent variable. \\n• \\nMultiple Linear Regression: Two or more independent variables. \\n \\n5. Evaluation Metrics \\n• \\nR-squared (R²): Explains how much variance in Y is explained by X. \\n• \\nMean Squared Error (MSE): Average squared difference between predicted and actual \\nvalues. \\n• \\nRoot Mean Squared Error (RMSE): Square root of MSE, easier to interpret.']]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1bcd0d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided text does not mention \"Multi Agent Architecture.\" It appears to be a guide on Linear Regression, a statistical method used to model the relationship between a dependent variable and one or more independent variables. \n",
      "\n",
      "If you are looking for information on Multi Agent Architecture, I can provide a general summary. Multi-Agent Architecture is a design pattern used in artificial intelligence and multi-agent systems. It involves designing a system that consists of multiple autonomous agents that interact with each other and their environment to achieve a common goal. The architecture typically includes:\n",
      "\n",
      "1. Agent definition: Each agent has its own goals, behaviors, and decision-making processes.\n",
      "2. Communication: Agents communicate with each other to share information and coordinate actions.\n",
      "3. Environment: The agents interact with their environment, which can be physical or virtual.\n",
      "4. Control: The system has a control mechanism that manages the interactions between agents and the environment.\n",
      "\n",
      "Multi-Agent Architecture is used in various applications, such as robotics, autonomous systems, and social networks. It allows for more flexible and adaptive systems that can respond to changing situations and learn from experience.\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\" Summarize the following for the query:'{query}'\\n\\n Context:\\n{texts}\\n\\nSummary:\"\"\"\n",
    "response = chat_groq_llm.invoke(prompt)\n",
    "print(response.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genaipractise",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
